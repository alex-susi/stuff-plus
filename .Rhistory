sampled_grid <- param_grid_fb[sample(nrow(param_grid_fb), 50), ]
cluster <- parallel::makeCluster(parallel::detectCores())
# Initialize results list
results_fb <- list()
dtrain_fb_sub <- dtrain_fb[sample(1:nrow(dtrain_fb), 100000), ]
# Perform grid search for sampled parameters
for (i in seq_len(nrow(sampled_grid))) {
params_tuned_fb <- as.list(sampled_grid[i, ])
params_tuned_fb <- append(params_tuned_fb, list(
booster = "gbtree",
objective = "reg:squarederror",
eval_metric = "rmse"
))
# Cross-validation with early stopping
cv_result <- xgb.cv(
params = params_tuned_fb,
data = dtrain_fb_sub,
nrounds = params_tuned_fb$nrounds,
nfold = 5,
early_stopping_rounds = 10,
verbose = 0
)
# Store results
results_fb[[i]] <- list(
params = params_tuned_fb,
rmse = min(cv_result$evaluation_log$test_rmse_mean)
)
}
parallel::stopCluster(cluster)
# Get the best parameters
best_params_fb <- do.call(rbind, lapply(results_fb, function(res) {
c(res$params, rmse = res$rmse)
})) %>%
as_tibble() %>%
arrange(as.numeric(rmse))
print(best_params_fb)
# Get the best parameters
best_params_fb <- do.call(rbind, lapply(results_fb, function(res) {
c(res$params, rmse = res$rmse)
})) %>%
as_tibble() %>%
arrange(as.numeric(rmse)) %>%
as.data.frame()
print(best_params_fb)
best_params_fb <- list(
#nrounds = best_params_fb$nrounds[[1]],
eta = best_params_fb$eta[[1]],
max_depth = best_params_fb$max_depth[[1]],
colsample_bytree = best_params_fb$colsample_bytree[[1]],
subsample = best_params_fb$subsample[[1]],
min_child_weight = best_params_fb$min_child_weight[[1]],
lambda = 1,
alpha = 1
)
# Re-run model with tuned parameters
xgb_model_fb_tuned <- xgb.train(
params = best_params_fb,
data = dtrain_fb,
nrounds = best_params_fb$nrounds[[1]],
watchlist = list(train = dtrain_fb),
print_every_n = 50
)
best_params_fb <- list(
nrounds = best_params_fb$nrounds[[1]],
eta = best_params_fb$eta[[1]],
max_depth = best_params_fb$max_depth[[1]],
colsample_bytree = best_params_fb$colsample_bytree[[1]],
subsample = best_params_fb$subsample[[1]],
min_child_weight = best_params_fb$min_child_weight[[1]],
lambda = 1,
alpha = 1
)
# Re-run model with tuned parameters
xgb_model_fb_tuned <- xgb.train(
params = best_params_fb,
data = dtrain_fb,
nrounds = best_params_fb$nrounds[[1]],
watchlist = list(train = dtrain_fb),
print_every_n = 50
)
dtrain_fb
best_params_fb <- list(
eta = best_params_fb$eta[[1]],
max_depth = best_params_fb$max_depth[[1]],
colsample_bytree = best_params_fb$colsample_bytree[[1]],
subsample = best_params_fb$subsample[[1]],
min_child_weight = best_params_fb$min_child_weight[[1]],
lambda = 1,
alpha = 1
)
# Re-run model with tuned parameters
xgb_model_fb_tuned <- xgb.train(
params = best_params_fb,
data = dtrain_fb,
nrounds = best_params_fb$nrounds[[1]],
watchlist = list(train = dtrain_fb),
print_every_n = 50
)
best_params_fb
best_params_fb <- list(
nrounds = best_params_fb$nrounds[[1]],
eta = best_params_fb$eta[[1]],
max_depth = best_params_fb$max_depth[[1]],
colsample_bytree = best_params_fb$colsample_bytree[[1]],
subsample = best_params_fb$subsample[[1]],
min_child_weight = best_params_fb$min_child_weight[[1]],
lambda = 1,
alpha = 1
)
best_params_fb
# Get the best parameters
best_params_fb <- do.call(rbind, lapply(results_fb, function(res) {
c(res$params, rmse = res$rmse)
})) %>%
as_tibble() %>%
arrange(as.numeric(rmse)) %>%
as.data.frame()
best_params_fb
best_params_fb <- list(
nrounds = best_params_fb$nrounds[[1]],
eta = best_params_fb$eta[[1]],
max_depth = best_params_fb$max_depth[[1]],
colsample_bytree = best_params_fb$colsample_bytree[[1]],
subsample = best_params_fb$subsample[[1]],
min_child_weight = best_params_fb$min_child_weight[[1]],
lambda = 1,
alpha = 1
)
best_params_fb
# Re-run model with tuned parameters
xgb_model_fb_tuned <- xgb.train(
params = best_params_fb,
data = dtrain_fb,
nrounds = best_params_fb$nrounds[[1]],
watchlist = list(train = dtrain_fb),
print_every_n = 50
)
# Tuned Feature Importance
importance_fb_tuned <- xgb.importance(feature_names = colnames(
model.matrix(~ . - 1, data = train_fb[, features])),
model = xgb_model_fb_tuned)
xgb.plot.importance(importance_fb_tuned)
# Filter the dataframe to include only rows for the year 2023 and 2024
test_fb_2023 <- test_fb %>%
filter(year == 2023) %>%
mutate(delta_run_exp_mean = xgb_model_fb_tuned %>%
predict(as.matrix(select(., all_of(features)))))
rlang::last_trace()
# Filter the dataframe to include only rows for the year 2023 and 2024
test_fb_2023 <- test_fb %>%
filter(year == 2023) %>%
mutate(delta_run_exp_mean = predict(xgb_model_fb_tuned,
newdata = as.matrix(select(., all_of(features)))))
test_fb_2024 <- test_fb %>%
filter(year == 2024) %>%
mutate(delta_run_exp_mean = predict(xgb_model_fb_tuned,
newdata = as.matrix(select(., all_of(features)))))
## 2023 Stuff+ ##
# Calculate the mean and standard deviation of the delta_run_exp_mean column for 2023
delta_run_exp_mean_mean <- mean(test_fb_2023$delta_run_exp_mean, na.rm = TRUE)
delta_run_exp_mean_std <- sd(test_fb_2023$delta_run_exp_mean, na.rm = TRUE)
# Standardize the delta_run_exp_mean column to create a z-score for 2023
test_fb_2023 <- test_fb_2023 %>%
mutate(
delta_run_exp_mean_zscore = (delta_run_exp_mean - delta_run_exp_mean_mean) /
delta_run_exp_mean_std,
stuff_plus = 100 - (delta_run_exp_mean_zscore * 10)
)
# Aggregate stuff_plus by pitcher_id and year for 2023
fb_agg_2023 <- test_fb_2023 %>%
group_by(pitcher_id, pitcher_name, year) %>%
summarise(
count = n(),
mean_stuff_plus = mean(stuff_plus, na.rm = TRUE),
.groups = "drop"
) %>%
as.data.frame()
## 2024 Stuff+ ##
# Standardize the delta_run_exp_mean column to create a z-score
# for 2024 using 2023 mean and std
test_fb_2024 <- test_fb_2024 %>%
mutate(
delta_run_exp_mean_zscore = (delta_run_exp_mean - delta_run_exp_mean_mean) /
delta_run_exp_mean_std,
stuff_plus = 100 - (delta_run_exp_mean_zscore * 10)
)
# Aggregate stuff_plus by pitcher_id and year for 2024
fb_agg_2024 <- test_fb_2024 %>%
group_by(pitcher_id, pitcher_name, year) %>%
summarise(
count = n(),
mean_stuff_plus = mean(stuff_plus, na.rm = TRUE),
.groups = "drop"
) %>%
as.data.frame()
test_fb_2023 %>% arrange(desc(stuff_plus)) %>% head()
hist(test_fb_2023$stuff_plus)
test_fb_2024 %>% arrange(desc(stuff_plus)) %>% head()
fb_agg_2023 %>% arrange(desc(mean_stuff_plus)) %>% head()
fb_agg_2024 %>% arrange(desc(mean_stuff_plus)) %>% head()
fb_agg_2024 %>% arrange((mean_stuff_plus)) %>% head()
# Aggregate stuff_plus by pitcher_id and year for 2023
fb_agg_2023 <- test_fb_2023 %>%
group_by(pitcher_id, pitcher_name, pitch_name, year) %>%
summarise(
count = n(),
mean_stuff_plus = mean(stuff_plus, na.rm = TRUE),
.groups = "drop"
) %>%
as.data.frame()
fb_agg_2023 %>% arrange(desc(mean_stuff_plus)) %>% head()
# Aggregate stuff_plus by pitcher_id and year for 2024
fb_agg_2024 <- test_fb_2024 %>%
group_by(pitcher_id, pitcher_name, pitch_name, year) %>%
summarise(
count = n(),
mean_stuff_plus = mean(stuff_plus, na.rm = TRUE),
.groups = "drop"
) %>%
as.data.frame()
fb_agg_2024 %>% arrange(desc(mean_stuff_plus)) %>% head()
fb_agg_2024 %>% arrange(desc(mean_stuff_plus)) %>% head(n = 10)
dtrain_bb <- xgb.DMatrix(data = model.matrix(~ . - 1, data = train_bb[, features]),
label = train_bb$delta_run_exp_mean)
# Train XGBoost model
xgb_model_bb <- xgb.train(
params = params,
data = dtrain_bb,
nrounds = 250,
watchlist = list(train = dtrain_bb),
print_every_n = 50
)
# Feature Importance
importance_bb <- xgb.importance(feature_names = colnames(
model.matrix(~ . - 1, data = train_bb[, features])),
model = xgb_model_bb)
xgb.plot.importance(importance_bb)
xgb.plot.importance(importance_fb)
xgb.plot.importance(importance_bb)
# Hyperparameter Tuning
param_grid_bb <- expand.grid(
nrounds = c(100, 250, 500),           # Reduced range for boosting rounds
eta = c(0.1, 0.2, 0.3),                      # Coarser learning rate grid
max_depth = c(2, 6, 10),                      # Key depths, avoiding very deep trees
colsample_bytree = c(0.5, 0.75, 1),          # Coarser grid for feature subsampling
subsample = c(0.6, 0.8, 1),                  # Focused range for row subsampling
min_child_weight = c(1, 10, 50),             # Coarse grid for minimum child weight
lambda = c(1),                               # Fixing L2 regularization
alpha = c(1)                                 # Fixing L1 regularization
)
# Randomly select 50 combinations
sampled_grid <- param_grid_bb[sample(nrow(param_grid_bb), 50), ]
cluster <- parallel::makeCluster(parallel::detectCores())
# Initialize results list
results_bb <- list()
dtrain_bb_sub <- dtrain_bb[sample(1:nrow(dtrain_bb), 100000), ]
# Perform grid search for sampled parameters
for (i in seq_len(nrow(sampled_grid))) {
params_tuned_bb <- as.list(sampled_grid[i, ])
params_tuned_bb <- append(params_tuned_bb, list(
booster = "gbtree",
objective = "reg:squarederror",
eval_metric = "rmse"
))
# Cross-validation with early stopping
cv_result <- xgb.cv(
params = params_tuned_bb,
data = dtrain_bb_sub,
nrounds = params_tuned_bb$nrounds,
nfold = 5,
early_stopping_rounds = 10,
verbose = 0
)
# Store results
results_bb[[i]] <- list(
params = params_tuned_bb,
rmse = min(cv_result$evaluation_log$test_rmse_mean)
)
}
parallel::stopCluster(cluster)
# Get the best parameters
best_params_bb <- do.call(rbind, lapply(results_bb, function(res) {
c(res$params, rmse = res$rmse)
})) %>%
as_tibble() %>%
arrange(as.numeric(rmse)) %>%
as.data.frame()
print(best_params_bb)
best_params_bb <- list(
nrounds = best_params_bb$nrounds[[1]],
eta = best_params_bb$eta[[1]],
max_depth = best_params_bb$max_depth[[1]],
colsample_bytree = best_params_bb$colsample_bytree[[1]],
subsample = best_params_bb$subsample[[1]],
min_child_weight = best_params_bb$min_child_weight[[1]],
lambda = 1,
alpha = 1
)
# Re-run model with tuned parameters
xgb_model_bb_tuned <- xgb.train(
params = best_params_bb,
data = dtrain_bb,
nrounds = best_params_bb$nrounds[[1]],
watchlist = list(train = dtrain_bb),
print_every_n = 50
)
# Tuned Feature Importance
importance_bb_tuned <- xgb.importance(feature_names = colnames(
model.matrix(~ . - 1, data = train_bb[, features])),
model = xgb_model_bb_tuned)
xgb.plot.importance(importance_bb_tuned)
# Filter the dataframe to include only rows for the year 2023 and 2024
test_bb_2023 <- test_bb %>%
filter(year == 2023) %>%
mutate(delta_run_exp_mean = predict(xgb_model_bb_tuned,
newdata = as.matrix(select(., all_of(features)))))
test_bb_2024 <- test_bb %>%
filter(year == 2024) %>%
mutate(delta_run_exp_mean = predict(xgb_model_bb_tuned,
newdata = as.matrix(select(., all_of(features)))))
## 2023 Stuff+ ##
# Calculate the mean and standard deviation of the delta_run_exp_mean column for 2023
bb_target_mean <- mean(test_bb_2023$delta_run_exp_mean, na.rm = TRUE)
bb_target_std <- sd(test_bb_2023$delta_run_exp_mean, na.rm = TRUE)
# Standardize the delta_run_exp_mean column to create a z-score for 2023
test_bb_2023 <- test_bb_2023 %>%
mutate(
delta_run_exp_mean_zscore = (delta_run_exp_mean - bb_target_mean) / bb_target_std,
stuff_plus = 100 - (delta_run_exp_mean_zscore * 10)
)
# Aggregate stuff_plus by pitcher_id and year for 2023
bb_agg_2023 <- test_bb_2023 %>%
group_by(pitcher_id, pitcher_name, pitch_name, year) %>%
summarise(
count = n(),
mean_stuff_plus = mean(stuff_plus, na.rm = TRUE),
.groups = "drop"
) %>%
as.data.frame()
## 2024 Stuff+ ##
# Standardize the delta_run_exp_mean column to create a z-score
# for 2024 using 2023 mean and std
test_bb_2024 <- test_bb_2024 %>%
mutate(
delta_run_exp_mean_zscore = (delta_run_exp_mean - bb_target_mean) / bb_target_std,
stuff_plus = 100 - (delta_run_exp_mean_zscore * 10)
)
# Aggregate stuff_plus by pitcher_id and year for 2024
bb_agg_2024 <- test_bb_2024 %>%
group_by(pitcher_id, pitcher_name, pitch_name, year) %>%
summarise(
count = n(),
mean_stuff_plus = mean(stuff_plus, na.rm = TRUE),
.groups = "drop"
) %>%
as.data.frame()
bb_agg_2023 %>% arrange(desc(mean_stuff_plus)) %>% head()
bb_agg_2024 %>% arrange(desc(mean_stuff_plus)) %>% head()
bb_agg_2024 %>% arrange(desc(mean_stuff_plus)) %>% head(n = 10)
bb_agg_2024 %>% arrange(desc(mean_stuff_plus)) %>% head(n = 20)
bb_agg_2024 %>% arrange(desc(mean_stuff_plus)) %>% head(n = 50)
test_bb_2024 %>% arrange(desc(stuff_plus)) %>% head()
hist(test_bb_2024$stuff_plus)
dtrain_off <- xgb.DMatrix(data = model.matrix(~ . - 1, data = train_off[, features]),
label = train_off$delta_run_exp_mean)
# Train XGBoost model
xgb_model_off <- xgb.train(
params = params,
data = dtrain_off,
nrounds = 250,
watchlist = list(train = dtrain_off),
print_every_n = 50
)
fb_agg_2024 %>% arrange(desc(mean_stuff_plus)) %>% head(n = 10)
fb_agg_2024 %>% arrange(desc(mean_stuff_plus)) %>% head(n = 20)
# Feature Importance
importance_off <- xgb.importance(feature_names = colnames(
model.matrix(~ . - 1, data = train_off[, features])),
model = xgb_model_off)
xgb.plot.importance(importance_off)
# Hyperparameter Tuning
param_grid_off <- expand.grid(
nrounds = c(100, 250, 500),           # Reduced range for boosting rounds
eta = c(0.1, 0.2, 0.3),                      # Coarser learning rate grid
max_depth = c(2, 6, 10),                      # Key depths, avoiding very deep trees
colsample_bytree = c(0.5, 0.75, 1),          # Coarser grid for feature subsampling
subsample = c(0.6, 0.8, 1),                  # Focused range for row subsampling
min_child_weight = c(1, 10, 50),             # Coarse grid for minimum child weight
lambda = c(1),                               # Fixing L2 regularization
alpha = c(1)                                 # Fixing L1 regularization
)
# Randomly select 50 combinations
sampled_grid <- param_grid_off[sample(nrow(param_grid_off), 50), ]
cluster <- parallel::makeCluster(parallel::detectCores())
# Initialize results list
results_off <- list()
dtrain_off_sub <- dtrain_off[sample(1:nrow(dtrain_off), 100000), ]
# Perform grid search for sampled parameters
for (i in seq_len(nrow(sampled_grid))) {
params_tuned_off <- as.list(sampled_grid[i, ])
params_tuned_off <- append(params_tuned_off, list(
booster = "gbtree",
objective = "reg:squarederror",
eval_metric = "rmse"
))
# Cross-validation with early stopping
cv_result <- xgb.cv(
params = params_tuned_off,
data = dtrain_off_sub,
nrounds = params_tuned_off$nrounds,
nfold = 5,
early_stopping_rounds = 10,
verbose = 0
)
# Store results
results_off[[i]] <- list(
params = params_tuned_off,
rmse = min(cv_result$evaluation_log$test_rmse_mean)
)
}
parallel::stopCluster(cluster)
# Get the best parameters
best_params_off <- do.call(rbind, lapply(results_off, function(res) {
c(res$params, rmse = res$rmse)
})) %>%
as_tioffle() %>%
arrange(as.numeric(rmse)) %>%
as.data.frame()
# Get the best parameters
best_params_off <- do.call(rbind, lapply(results_off, function(res) {
c(res$params, rmse = res$rmse)
})) %>%
arrange(as.numeric(rmse)) %>%
as.data.frame()
# Get the best parameters
best_params_off <- do.call(rbind, lapply(results_off, function(res) {
c(res$params, rmse = res$rmse)
})) %>%
as_tibble() %>%
arrange(as.numeric(rmse)) %>%
as.data.frame()
print(best_params_off)
best_params_off <- list(
nrounds = best_params_off$nrounds[[1]],
eta = best_params_off$eta[[1]],
max_depth = best_params_off$max_depth[[1]],
colsample_bytree = best_params_off$colsample_bytree[[1]],
subsample = best_params_off$subsample[[1]],
min_child_weight = best_params_off$min_child_weight[[1]],
lambda = 1,
alpha = 1
)
# Re-run model with tuned parameters
xgb_model_off_tuned <- xgb.train(
params = best_params_off,
data = dtrain_off,
nrounds = best_params_off$nrounds[[1]],
watchlist = list(train = dtrain_off),
print_every_n = 50
)
# Tuned Feature Importance
importance_off_tuned <- xgb.importance(feature_names = colnames(
model.matrix(~ . - 1, data = train_off[, features])),
model = xgb_model_off_tuned)
xgb.plot.importance(importance_off_tuned)
# Filter the dataframe to include only rows for the year 2023 and 2024
test_off_2023 <- test_off %>%
filter(year == 2023) %>%
mutate(delta_run_exp_mean = predict(xgb_model_off_tuned,
newdata = as.matrix(select(., all_of(features)))))
test_off_2024 <- test_off %>%
filter(year == 2024) %>%
mutate(delta_run_exp_mean = predict(xgb_model_off_tuned,
newdata = as.matrix(select(., all_of(features)))))
## 2023 Stuff+ ##
# Calculate the mean and standard deviation of the delta_run_exp_mean column for 2023
off_target_mean <- mean(test_off_2023$delta_run_exp_mean, na.rm = TRUE)
off_target_std <- sd(test_off_2023$delta_run_exp_mean, na.rm = TRUE)
# Standardize the delta_run_exp_mean column to create a z-score for 2023
test_off_2023 <- test_off_2023 %>%
mutate(
delta_run_exp_mean_zscore = (delta_run_exp_mean - off_target_mean) / off_target_std,
stuff_plus = 100 - (delta_run_exp_mean_zscore * 10)
)
# Aggregate stuff_plus by pitcher_id and year for 2023
off_agg_2023 <- test_off_2023 %>%
group_by(pitcher_id, pitcher_name, pitch_name, year) %>%
summarise(
count = n(),
mean_stuff_plus = mean(stuff_plus, na.rm = TRUE),
.groups = "drop"
) %>%
as.data.frame()
## 2024 Stuff+ ##
# Standardize the delta_run_exp_mean column to create a z-score
# for 2024 using 2023 mean and std
test_off_2024 <- test_off_2024 %>%
mutate(
delta_run_exp_mean_zscore = (delta_run_exp_mean - off_target_mean) / off_target_std,
stuff_plus = 100 - (delta_run_exp_mean_zscore * 10)
)
# Aggregate stuff_plus by pitcher_id and year for 2024
off_agg_2024 <- test_off_2024 %>%
group_by(pitcher_id, pitcher_name, pitch_name, year) %>%
summarise(
count = n(),
mean_stuff_plus = mean(stuff_plus, na.rm = TRUE),
.groups = "drop"
) %>%
as.data.frame()
off_agg_2024 %>% arrange(desc(mean_stuff_plus)) %>% head(n = 10)
test_off_2024 %>% arrange(desc(stuff_plus)) %>% head(n = 5)
